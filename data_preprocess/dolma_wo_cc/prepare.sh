nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/algebraic-stack-train-0000.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/algebraic-stack-train-0000 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/algebraic-stack-train-0001.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/algebraic-stack-train-0001 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/algebraic-stack-train-0002.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/algebraic-stack-train-0002 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/algebraic-stack-train-0003.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/algebraic-stack-train-0003 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/algebraic-stack-train-0004.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/algebraic-stack-train-0004 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/algebraic-stack-train-0005.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/algebraic-stack-train-0005 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/algebraic-stack-train-0006.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/algebraic-stack-train-0006 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/algebraic-stack-train-0007.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/algebraic-stack-train-0007 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/algebraic-stack-train-0008.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/algebraic-stack-train-0008 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/algebraic-stack-train-0009.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/algebraic-stack-train-0009 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/algebraic-stack-train-0010.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/algebraic-stack-train-0010 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/algebraic-stack-train-0011.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/algebraic-stack-train-0011 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/algebraic-stack-train-0012.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/algebraic-stack-train-0012 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/algebraic-stack-train-0013.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/algebraic-stack-train-0013 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/algebraic-stack-train-0014.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/algebraic-stack-train-0014 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/algebraic-stack-train-0015.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/algebraic-stack-train-0015 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0000.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0000 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0001.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0001 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0002.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0002 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0003.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0003 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0004.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0004 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0005.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0005 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0006.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0006 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0007.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0007 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0008.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0008 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0009.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0009 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0010.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0010 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0011.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0011 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0012.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0012 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0013.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0013 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0014.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0014 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0015.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0015 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0016.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0016 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0017.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0017 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0018.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0018 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0019.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0019 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0020.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0020 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0021.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0021 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0022.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0022 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0023.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0023 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0024.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0024 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0025.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0025 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0026.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0026 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0027.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0027 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0028.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0028 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0029.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0029 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0030.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0030 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0031.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0031 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0032.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0032 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0033.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0033 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0034.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0034 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0035.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0035 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0036.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0036 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0037.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0037 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0038.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0038 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0039.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0039 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0040.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0040 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0041.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0041 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0042.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0042 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0043.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0043 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0044.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0044 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0045.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0045 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0046.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0046 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0047.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0047 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0048.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0048 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0049.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0049 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0050.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0050 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0051.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0051 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0052.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0052 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0053.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0053 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0054.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0054 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0055.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0055 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0056.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0056 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0057.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0057 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0058.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0058 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0059.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0059 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0060.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0060 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0061.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0061 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0062.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0062 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0063.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0063 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0064.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0064 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0065.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0065 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0066.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0066 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0067.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0067 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0068.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0068 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0069.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0069 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0070.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0070 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0071.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0071 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0072.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0072 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0073.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0073 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0074.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0074 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0075.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0075 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0076.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0076 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0077.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0077 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0078.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0078 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0079.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0079 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0080.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0080 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0081.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0081 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0082.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0082 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0083.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0083 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0084.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0084 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &

sleep 15m

nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0085.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0085 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0086.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0086 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0087.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0087 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0088.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0088 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0089.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0089 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0090.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0090 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0091.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0091 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0092.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0092 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0093.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0093 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0094.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0094 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0095.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0095 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0096.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0096 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0097.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0097 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0098.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0098 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/arxiv-0099.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/arxiv-0099 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/books-0000.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/books-0000 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/books-0001.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/books-0001 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/books-0002.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/books-0002 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0000.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0000 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0001.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0001 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0002.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0002 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0003.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0003 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0004.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0004 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0005.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0005 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0006.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0006 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0007.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0007 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0008.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0008 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0009.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0009 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0010.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0010 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0011.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0011 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0012.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0012 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0013.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0013 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0014.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0014 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0015.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0015 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0016.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0016 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0017.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0017 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0018.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0018 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0019.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0019 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0020.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0020 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0021.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0021 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0022.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0022 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0023.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0023 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0024.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0024 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0025.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0025 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0026.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0026 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0027.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0027 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0028.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0028 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0029.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0029 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0030.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0030 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0031.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0031 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0032.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0032 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0033.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0033 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0034.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0034 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0035.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0035 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0036.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0036 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0037.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0037 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0038.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0038 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0039.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0039 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0040.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0040 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0041.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0041 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0042.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0042 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0043.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0043 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0044.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0044 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0045.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0045 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0046.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0046 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0047.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0047 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0048.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0048 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0049.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0049 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0050.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0050 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0051.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0051 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0052.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0052 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0053.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0053 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0054.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0054 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0055.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0055 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0056.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0056 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0057.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0057 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0058.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0058 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0059.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0059 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0060.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0060 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0061.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0061 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0062.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0062 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0063.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0063 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0064.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0064 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0065.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0065 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0066.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0066 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0067.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0067 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0068.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0068 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0069.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0069 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0070.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0070 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0071.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0071 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0072.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0072 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0073.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0073 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0074.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0074 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0075.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0075 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0076.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0076 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0077.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0077 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0078.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0078 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0079.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0079 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0080.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0080 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0081.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0081 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &

sleep 15m

nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0082.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0082 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0083.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0083 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0084.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0084 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0085.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0085 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0086.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0086 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0087.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0087 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0088.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0088 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0089.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0089 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0090.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0090 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0091.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0091 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0092.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0092 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0093.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0093 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0094.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0094 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0095.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0095 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0096.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0096 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0097.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0097 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0098.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0098 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0099.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0099 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0100.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0100 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0101.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0101 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0102.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0102 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0103.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0103 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0104.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0104 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0105.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0105 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0106.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0106 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0107.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0107 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0108.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0108 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0109.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0109 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0110.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0110 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0111.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0111 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0112.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0112 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0113.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0113 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0114.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0114 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0115.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0115 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0116.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0116 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0117.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0117 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0118.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0118 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0119.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0119 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0120.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0120 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0121.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0121 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0122.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0122 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0123.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0123 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0124.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0124 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0125.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0125 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0126.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0126 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0127.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0127 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0128.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0128 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0129.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0129 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0130.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0130 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0131.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0131 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0132.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0132 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0133.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0133 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0134.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0134 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0135.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0135 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0136.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0136 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0137.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0137 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0138.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0138 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0139.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0139 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0140.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0140 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0141.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0141 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0142.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0142 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0143.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0143 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0144.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0144 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0145.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0145 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0146.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0146 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0147.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0147 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0148.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0148 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0149.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0149 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0150.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0150 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0151.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0151 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0152.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0152 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0153.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0153 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0154.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0154 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0155.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0155 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0156.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0156 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0157.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0157 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0158.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0158 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0159.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0159 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0160.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0160 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0161.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0161 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0162.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0162 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0163.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0163 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0164.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0164 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0165.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0165 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0166.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0166 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0167.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0167 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0168.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0168 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0169.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0169 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0170.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0170 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0171.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0171 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0172.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0172 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0173.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0173 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0174.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0174 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0175.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0175 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0176.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0176 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0177.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0177 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0178.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0178 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0179.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0179 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0180.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0180 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0181.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0181 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &

sleep 15m

nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0182.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0182 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0183.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0183 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0184.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0184 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0185.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0185 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0186.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0186 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0187.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0187 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0188.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0188 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0189.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0189 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0190.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0190 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0191.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0191 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0192.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0192 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0193.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0193 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0194.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0194 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0195.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0195 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0196.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0196 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0197.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0197 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0198.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0198 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0199.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0199 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0200.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0200 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0201.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0201 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0202.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0202 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0203.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0203 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0204.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0204 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0205.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0205 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0206.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0206 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0207.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0207 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0208.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0208 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0209.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0209 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0210.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0210 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0211.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0211 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0212.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0212 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0213.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0213 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0214.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0214 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0215.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0215 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0216.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0216 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0217.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0217 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0218.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0218 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0219.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0219 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0220.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0220 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0221.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0221 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0222.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0222 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0223.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0223 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0224.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0224 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0225.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0225 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0226.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0226 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0227.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0227 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0228.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0228 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0229.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0229 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0230.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0230 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0231.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0231 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0232.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0232 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0233.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0233 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0234.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0234 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0235.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0235 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0236.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0236 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0237.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0237 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0238.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0238 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0239.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0239 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0240.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0240 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0241.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0241 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0242.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0242 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0243.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0243 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0244.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0244 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0245.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0245 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0246.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0246 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0247.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0247 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0248.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0248 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0249.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0249 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0250.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0250 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0251.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0251 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0252.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0252 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0253.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0253 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0254.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0254 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0255.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0255 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0256.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0256 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0257.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0257 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0258.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0258 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0259.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0259 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0260.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0260 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/megawika-0261.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/megawika-0261 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/open-web-math-train-0000.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/open-web-math-train-0000 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/open-web-math-train-0001.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/open-web-math-train-0001 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/open-web-math-train-0002.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/open-web-math-train-0002 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/open-web-math-train-0003.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/open-web-math-train-0003 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/open-web-math-train-0004.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/open-web-math-train-0004 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/open-web-math-train-0005.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/open-web-math-train-0005 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/open-web-math-train-0006.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/open-web-math-train-0006 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/open-web-math-train-0007.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/open-web-math-train-0007 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/open-web-math-train-0008.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/open-web-math-train-0008 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/open-web-math-train-0009.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/open-web-math-train-0009 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/open-web-math-train-0010.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/open-web-math-train-0010 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/open-web-math-train-0011.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/open-web-math-train-0011 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/open-web-math-train-0012.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/open-web-math-train-0012 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/pes2o-0000.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/pes2o-0000 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/pes2o-0001.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/pes2o-0001 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/pes2o-0002.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/pes2o-0002 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/pes2o-0003.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/pes2o-0003 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/pes2o-0004.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/pes2o-0004 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/pes2o-0005.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/pes2o-0005 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/pes2o-0006.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/pes2o-0006 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &

sleep 15m

nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/pes2o-0007.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/pes2o-0007 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/pes2o-0008.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/pes2o-0008 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/pes2o-0009.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/pes2o-0009 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/pes2o-0010.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/pes2o-0010 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/pes2o-0011.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/pes2o-0011 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/pes2o-0012.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/pes2o-0012 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/pes2o-0013.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/pes2o-0013 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/pes2o-0014.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/pes2o-0014 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/pes2o-0015.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/pes2o-0015 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/pes2o-0016.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/pes2o-0016 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/pes2o-0017.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/pes2o-0017 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/pes2o-0018.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/pes2o-0018 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/pes2o-0019.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/pes2o-0019 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/pes2o-0020.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/pes2o-0020 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/pes2o-0021.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/pes2o-0021 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/pes2o-0022.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/pes2o-0022 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/pes2o-0023.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/pes2o-0023 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/pes2o-0024.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/pes2o-0024 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/pes2o-0025.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/pes2o-0025 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0000.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0000 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0001.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0001 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0002.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0002 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0003.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0003 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0004.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0004 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0005.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0005 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0006.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0006 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0007.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0007 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0008.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0008 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0009.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0009 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0010.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0010 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0011.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0011 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0012.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0012 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0013.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0013 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0014.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0014 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0015.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0015 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0016.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0016 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0017.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0017 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0018.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0018 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0019.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0019 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0020.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0020 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0021.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0021 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0022.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0022 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0023.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0023 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0024.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0024 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0025.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0025 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0026.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0026 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0027.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0027 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0028.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0028 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0029.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0029 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0030.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0030 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0031.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0031 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0032.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0032 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0033.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0033 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0034.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0034 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0035.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0035 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0036.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0036 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0037.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0037 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0038.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0038 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0039.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0039 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0040.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0040 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0041.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0041 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0042.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0042 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0043.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0043 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0044.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0044 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0045.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0045 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0046.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0046 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0047.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0047 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0048.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0048 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0049.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0049 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0050.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0050 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0051.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0051 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0052.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0052 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0053.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0053 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0054.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0054 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0055.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0055 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0056.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0056 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0057.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0057 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0058.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0058 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0059.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0059 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0060.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0060 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0061.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0061 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0062.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0062 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0063.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0063 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0064.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0064 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0065.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0065 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0066.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0066 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0067.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0067 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0068.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0068 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0069.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0069 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0070.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0070 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0071.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0071 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0072.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0072 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0073.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0073 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0074.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0074 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0075.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0075 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0076.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0076 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/reddit-0077.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/reddit-0077 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/stackexchange-0000.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/stackexchange-0000 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/stackexchange-0001.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/stackexchange-0001 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/stackexchange-0002.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/stackexchange-0002 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &

sleep 15m

nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/stackexchange-0003.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/stackexchange-0003 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/stackexchange-0004.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/stackexchange-0004 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/stackexchange-0005.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/stackexchange-0005 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/stackexchange-0006.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/stackexchange-0006 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/stackexchange-0007.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/stackexchange-0007 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/stackexchange-0008.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/stackexchange-0008 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/stackexchange-0009.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/stackexchange-0009 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/stackexchange-0010.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/stackexchange-0010 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/stackexchange-0011.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/stackexchange-0011 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/stackexchange-0012.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/stackexchange-0012 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/stackexchange-0013.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/stackexchange-0013 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/stackexchange-0014.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/stackexchange-0014 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/stackexchange-0015.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/stackexchange-0015 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/stackexchange-0016.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/stackexchange-0016 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/stackexchange-0017.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/stackexchange-0017 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/stackexchange-0018.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/stackexchange-0018 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/stackexchange-0019.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/stackexchange-0019 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/stackexchange-0020.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/stackexchange-0020 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/stackexchange-0021.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/stackexchange-0021 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/stackexchange-0022.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/stackexchange-0022 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/stackexchange-0023.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/stackexchange-0023 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/stackexchange-0024.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/stackexchange-0024 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/stackexchange-0025.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/stackexchange-0025 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0000.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0000 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0001.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0001 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0002.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0002 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0003.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0003 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0004.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0004 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0005.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0005 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0006.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0006 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0007.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0007 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0008.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0008 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0009.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0009 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0010.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0010 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0011.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0011 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0012.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0012 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0013.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0013 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0014.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0014 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0015.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0015 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0016.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0016 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0017.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0017 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0018.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0018 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0019.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0019 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0020.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0020 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0021.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0021 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0022.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0022 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0023.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0023 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0024.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0024 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0025.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0025 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0026.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0026 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0027.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0027 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0028.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0028 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0029.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0029 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0030.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0030 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0031.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0031 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0032.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0032 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0033.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0033 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0034.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0034 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0035.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0035 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0036.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0036 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0037.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0037 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0038.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0038 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0039.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0039 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0040.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0040 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0041.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0041 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0042.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0042 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0043.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0043 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0044.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0044 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0045.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0045 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0046.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0046 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0047.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0047 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/starcoder-0048.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/starcoder-0048 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0000.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0000 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0001.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0001 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0002.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0002 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0003.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0003 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0004.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0004 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0005.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0005 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0006.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0006 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0007.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0007 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0008.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0008 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0009.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0009 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0010.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0010 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0011.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0011 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0012.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0012 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0013.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0013 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0014.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0014 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0015.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0015 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0016.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0016 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0017.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0017 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0018.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0018 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0019.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0019 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0020.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0020 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0021.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0021 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0022.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0022 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0023.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0023 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0024.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0024 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0025.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0025 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0026.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0026 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0027.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0027 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &

sleep 15m

nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0028.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0028 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0029.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0029 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0030.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0030 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0031.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0031 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0032.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0032 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0033.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0033 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0034.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0034 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0035.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0035 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0036.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0036 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0037.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0037 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0038.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0038 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0039.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0039 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0040.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0040 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0041.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0041 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0042.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0042 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0043.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0043 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0044.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0044 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0045.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0045 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0046.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0046 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0047.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0047 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0048.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0048 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0049.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0049 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0050.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0050 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0051.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0051 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0052.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0052 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0053.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0053 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0054.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0054 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0055.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0055 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0056.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0056 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0057.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0057 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0058.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0058 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0059.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0059 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0060.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0060 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0061.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0061 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0062.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0062 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0063.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0063 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0064.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0064 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/tulu_flan-0065.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/tulu_flan-0065 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/wiki-0000.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/wiki-0000 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
nohup python tools/preprocess_data.py --input /data6/datasets/dolma/target/wiki-0001.json --output-prefix /data9/datasets/pretrain/dolma_wo_cc/wiki-0001 --tokenizer-model tokenizers/deepseekv3 --tokenizer-type HuggingFaceTokenizer --append-eod --json-keys text --workers 2 &
